# mlflow_tracking/docker-compose.yml


services:
  mlflow-server:
    build:
      context: .
      dockerfile: mlflow/Dockerfile.server
    ports:
      - "5000:5000"
    volumes:
      # Mount mlruns outside the container to persist data
      - ./mlruns:/mlruns
      # If you need persistent model registry metadata (SQLAlchemy DB), mount that too
      # - ./mlflow_db:/db
    networks:
      - mlflow-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000"]
      interval: 10s # Check more frequently during startup
      timeout: 5s
      retries: 5
      start_period: 30s # Give it time to start before first check

  app:
    build:
      context: .
      dockerfile: mlflow/Dockerfile.app
    # GIT_PYTHON_REFRESH=quiet is often needed in containers for git operations (like by kagglehub)
    environment:
      - GIT_PYTHON_REFRESH=quiet
      # Pass the MLflow URI to the app container for training/logging
      - MLFLOW_TRACKING_URI=http://mlflow-server:5000
      # Optionally pass Kaggle credentials as environment variables if needed by kagglehub
      # - KAGGLE_USERNAME=<your_username>
      # - KAGGLE_KEY=<your_key>
    # The app depends on the MLflow server being healthy to log runs
    depends_on:
      mlflow-server:
        condition: service_healthy
    volumes:
      # Mount mlruns so the app can write training artifacts
      - ./mlruns:/mlruns
      # Mount outputs to persist generated files (predictions, plots)
      - ./outputs:/app/outputs
    networks:
      - mlflow-network
    # Consider a restart policy if the app should retry training on failure
    # restart: on-failure

  bentoml:
    build:
      context: .
      # Ensure the correct path to the BentoML Dockerfile
      dockerfile: Dockerfile.bentoml
    # Restart policy: 'always' means it will try to restart if it fails,
    # useful if the MLflow model isn't ready on the first try.
    restart: always
    ports:
      - "3000:3000"
    # BentoML service depends on the MLflow server to import the model at startup
    depends_on:
      mlflow-server:
        condition: service_healthy
      # Also, the BentoML service *logically* depends on the 'app' service
      # having trained and promoted a model to the 'Production' alias.
      # docker-compose `depends_on` only ensures startup order/health, not logical state.
      # The entrypoint script includes error handling if the model isn't found.
      # You *could* add 'app' here with a suitable condition if needed, e.g.,
      # app:
      #   condition: service_completed_successfully # If app runs once and stops
    volumes:
      # Mount mlruns - primarily for visibility or if BentoML needs to read directly (import_model uses URI)
      - ./mlruns:/mlruns
      # Mount the BentoML home directory to persist models and bentos *within* the container
      # This volume is critical for the entrypoint script to import models *into* the container's store.
      # Map a local folder (e.g., ./bentoml_home) or named volume to /root/.bentoml
      - ./bentoml_home:/root/.bentoml # Use a dedicated folder for BentoML data

      # The outputs volume might not be needed in the serving container unless
      # the service itself writes outputs.
      # - ./outputs:/app/outputs

    environment:
      # Pass MLflow Tracking URI to the entrypoint script and potentially the service code
      - MLFLOW_TRACKING_URI=http://mlflow-server:5000
      # Pass parameters to the entrypoint script for model import
      - BENTOML_MODEL_TO_SERVE_NAME=tracking-wine-logisticregression # The MLflow registered model name
      - BENTOML_MODEL_TO_SERVE_ALIAS=Production # The MLflow alias to import (e.g., Staging, Production)
      - BENTOML_MODEL_IMPORT_PREFIX=wine_service_model # Prefix for the resulting BentoML model name

      # Optionally pass the service definition path if not hardcoded in entrypoint
      # - BENTOML_SERVICE_DEFINITION=./bentoml/service.py:svc

    networks:
      - mlflow-network

networks:
  mlflow-network:
    driver: bridge

volumes:
  mlruns: # Define mlruns as a named volume if you prefer over bind mount
    driver: local
  bentoml_home: # Define a named volume for BentoML data
    driver: local
  outputs: # Define outputs as a named volume if you prefer
    driver: local

# You can remove the volume definitions above if you are using bind mounts (./folder:/path)
# instead of named volumes. The current docker-compose uses bind mounts for mlruns and outputs,
# and I've added a bind mount for bentoml_home. You can switch to named volumes if preferred.